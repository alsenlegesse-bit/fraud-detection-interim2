{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection - Model Building (Interim 2)\\n",
    "## Task 2: Model Building and Training\\n",
    "\\n",
    "This notebook implements the model building and training for fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\\n",
    "import numpy as np\\n",
    "import matplotlib.pyplot as plt\\n",
    "import seaborn as sns\\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\\n",
    "from sklearn.preprocessing import StandardScaler\\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\\n",
    "import warnings\\n",
    "warnings.filterwarnings(\"ignore\")\\n",
    "\\n",
    "# Set style\\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\\n",
    "*(Note: For demonstration, we create synthetic data. Replace with actual data loading)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data for demonstration\\n",
    "np.random.seed(42)\\n",
    "n_samples = 10000\\n",
    "n_fraud = int(n_samples * 0.015)  # 1.5% fraud rate\\n",
    "\\n",
    "# Generate features\\n",
    "X = np.random.randn(n_samples, 10)\\n",
    "y = np.zeros(n_samples)\\n",
    "y[:n_fraud] = 1  # First n_fraud samples are fraud\\n",
    "np.random.shuffle(y)\\n",
    "\\n",
    "# Split data\\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\\n",
    ")\\n",
    "\\n",
    "print(f\"Training set: {X_train.shape}\")\\n",
    "print(f\"Test set: {X_test.shape}\")\\n",
    "print(f\"Fraud rate in training: {y_train.mean():.3%}\")\\n",
    "print(f\"Fraud rate in test: {y_test.mean():.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handle Class Imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\\n",
    "\\n",
    "print(\"Class distribution before SMOTE:\")\\n",
    "print(pd.Series(y_train).value_counts())\\n",
    "\\n",
    "smote = SMOTE(random_state=42)\\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\\n",
    "\\n",
    "print(\"\\\\nClass distribution after SMOTE:\")\\n",
    "print(pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Baseline Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\\n",
    "from src.train import evaluate_model\\n",
    "\\n",
    "# Train model\\n",
    "lr_model = LogisticRegression(\\n",
    "    class_weight=\\\"balanced\\\",\\n",
    "    random_state=42,\\n",
    "    max_iter=1000\\n",
    ")\\n",
    "lr_model.fit(X_train_resampled, y_train_resampled)\\n",
    "\\n",
    "# Evaluate\\n",
    "lr_results = evaluate_model(lr_model, X_test, y_test, \\\"Logistic Regression\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Ensemble Model (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\\n",
    "\\n",
    "# Calculate scale_pos_weight for imbalance\\n",
    "scale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1])\\n",
    "\\n",
    "# Train XGBoost\\n",
    "xgb_model = XGBClassifier(\\n",
    "    n_estimators=200,\\n",
    "    max_depth=5,\\n",
    "    learning_rate=0.1,\\n",
    "    scale_pos_weight=scale_pos_weight,\\n",
    "    random_state=42,\\n",
    "    eval_metric=\\\"aucpr\\\",\\n",
    "    use_label_encoder=False\\n",
    ")\\n",
    "xgb_model.fit(X_train_resampled, y_train_resampled)\\n",
    "\\n",
    "# Evaluate\\n",
    "xgb_results = evaluate_model(xgb_model, X_test, y_test, \\\"XGBoost\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\\n",
    "\\n",
    "# Define cross-validation strategy\\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n",
    "\\n",
    "# Cross-validate XGBoost\\n",
    "xgb_cv_scores = cross_val_score(\\n",
    "    xgb_model, X_train_resampled, y_train_resampled,\\n",
    "    cv=cv, scoring=\\\"average_precision\\\"\\n",
    ")\\n",
    "\\n",
    "print(f\"XGBoost Cross-Validation AUC-PR Scores: {xgb_cv_scores}\")\\n",
    "print(f\"Mean: {xgb_cv_scores.mean():.4f}, Std: {xgb_cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\\n",
    "comparison_df = pd.DataFrame({\\n",
    "    \"Model\": [\\\"Logistic Regression\\\", \\\"XGBoost\\\"],\\n",
    "    \"AUC-PR\": [lr_results[\\\"auc_pr\\\"], xgb_results[\\\"auc_pr\\\"]],\\n",
    "    \"F1-Score\": [lr_results[\\\"f1_score\\\"], xgb_results[\\\"f1_score\\\"]]\\n",
    "})\\n",
    "\\n",
    "print(\"Model Performance Comparison:\")\\n",
    "print(comparison_df)\\n",
    "\\n",
    "# Visual comparison\\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\\n",
    "\\n",
    "# AUC-PR comparison\\n",
    "axes[0].bar(comparison_df[\\\"Model\\\"], comparison_df[\\\"AUC-PR\\\"], color=[\\\"skyblue\\\", \\\"lightcoral\\\"])\\n",
    "axes[0].set_title(\\\"AUC-PR Comparison\\\")\\n",
    "axes[0].set_ylabel(\\\"AUC-PR Score\\\")\\n",
    "axes[0].tick_params(axis=\\\"x\\\", rotation=45)\\n",
    "\\n",
    "# F1-Score comparison\\n",
    "axes[1].bar(comparison_df[\\\"Model\\\"], comparison_df[\\\"F1-Score\\\"], color=[\\\"skyblue\\\", \\\"lightcoral\\\"])\\n",
    "axes[1].set_title(\\\"F1-Score Comparison\\\")\\n",
    "axes[1].set_ylabel(\\\"F1-Score\\\")\\n",
    "axes[1].tick_params(axis=\\\"x\\\", rotation=45)\\n",
    "\\n",
    "plt.tight_layout()\\n",
    "plt.show()\\n",
    "\\n",
    "print(\"\\\\nâœ… Best Model Selected: XGBoost (Higher AUC-PR and F1-Score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\\n",
    "\\n",
    "# Save the best model\\n",
    "joblib.dump(xgb_model, \\\"models/xgb_fraud_model.pkl\\\")\\n",
    "print(\\\"Best model saved to models/xgb_fraud_model.pkl\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
